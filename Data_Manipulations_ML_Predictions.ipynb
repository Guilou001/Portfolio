{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ECO930J : Données massives et apprentissage automatique avec applications en économie\n",
    "TP \n",
    "\n",
    "Preprocessing data, compute ML estimators and ouput results table.\n",
    "\n",
    "15 avril 2022\n",
    "\n",
    "@author: LALS12039506, AGHN21599906, VAUG30119904, UQAM\n",
    "\"\"\"\n",
    "\n",
    "# Import relevant libraries\n",
    "import copy \n",
    "import numpy  as np\n",
    "import pandas as pd          \n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from statistics import mean\n",
    "\n",
    "# ML techniques\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.preprocessing   import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.linear_model    import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, \\\n",
    "                                    ElasticNetCV\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network  import MLPRegressor\n",
    "\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Location/path of project\n",
    "os.chdir('c:/Users/Poste/Documents/School/Maitrise Economique UQAM/'  \\\n",
    "         'ECO930J-Données massives et apprentissage automatique avec/TP')\n",
    "\n",
    "### Subfolders to create in project ###\n",
    "newpath = 'output/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "newpath = 'data/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "paths = ('data/', 'output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to sklearn documentation, alpha and lambda are reversed in the code compare to project statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "#### Here we will define two class that will be responsible for data manipulation and compouting ML estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    \"\"\"\n",
    "    Class taking for imput a complete dataset and preprocessing it for ML estimators.\n",
    "   \n",
    "    Warnings\n",
    "    ----------\n",
    "    The class does no validation for inputs in its initialization or in its methods. \n",
    "    It also does not ensure that the steps are performed in the correct order.\n",
    "    There is no exception handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,raw_data, var_names, model_name):\n",
    "        \"\"\"\n",
    "        Initialize an instance of this class.        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_data   : pandas.DataFrame\n",
    "                     Complete dataset for manipulation\n",
    "        var_names  : List(string)\n",
    "                     Names of the variables in dataset\n",
    "        model_name : string\n",
    "                     Name of the dataset   \n",
    "        \"\"\"        \n",
    "        self.model_name = model_name                # Name of the dataset  \n",
    "        self.var_names  = var_names                 # Names of the variables\n",
    "         \n",
    "        self.raw_data   = raw_data                  # Raw dataset(untouched)\n",
    "        self.data       = copy.deepcopy(raw_data)   # Dataset for manipulations     \n",
    "        \n",
    "        self.num_cols         = None                # Numerical variables\n",
    "        self.dic_cols         = None                # Dichotomic variables\n",
    "        self.cat_cols         = None                # Categorical variables\n",
    "        self.one_hot_cat_cols = None                # Dummies variables from categorical\n",
    "        \n",
    "        self.y          = pd.DataFrame({'A' : []})  # Outcome variable      \n",
    "        self.y_test     = pd.DataFrame({'A' : []})  # Test set of y\n",
    "        self.y_train    = pd.DataFrame({'A' : []})  # Train set of y\n",
    "        self.y_test_var = None                      # Variance of test set of y\n",
    "        self.y_name     = None                      # Names of outcome variable\n",
    "           \n",
    "        self.X       = pd.DataFrame({'A' : []})     # Predictors variables\n",
    "        self.X_test  = pd.DataFrame({'A' : []})     # Test set of X\n",
    "        self.X_train = pd.DataFrame({'A' : []})     # Train set of X\n",
    "        self.X_names = None                         # Names of predictors variables\n",
    "        \n",
    "    \n",
    "    def get_feature_name(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        X_names : List(string)\n",
    "                  Names of predictors variables\n",
    "        \"\"\"\n",
    "        return self.X_names    \n",
    "    \n",
    "        \n",
    "    def check_missing(self):\n",
    "        \"\"\"\n",
    "        Verify if there are missing values in the dataset.\n",
    "        \"\"\"\n",
    "        mat_bool  = self.data.isnull()        \n",
    "        check     = mat_bool.any(axis=None)        \n",
    "                \n",
    "        if check == True:      \n",
    "            \n",
    "            num = mat_bool.sum().sum()\n",
    "            print(self.model_name + f' contain {num} missing values')\n",
    "        else:\n",
    "            print(self.model_name + ' don\\'t contain missing values')\n",
    "        \n",
    "            \n",
    "    def seperate_num_cat(self):\n",
    "        \"\"\"\n",
    "        Separate variables into categorical, numerical and dichotomous.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        This method should only be used once by instance. \n",
    "        \"\"\"        \n",
    "        self.num_cols = [col for col in self.data.columns if \\\n",
    "                         self.data[col].dtype in ['float64','int64']]\n",
    "        \n",
    "        self.dic_cols = []\n",
    "        for n in self.num_cols:\n",
    "            col  = self.data[n].unique() \n",
    "            \n",
    "            if np.array_equal(col, np.array([0,1])):\n",
    "                self.dic_cols.append(n)\n",
    "                self.num_cols.remove(n)\n",
    "        \n",
    "        self.cat_cols = [col for col in self.data.columns if \\\n",
    "                         self.data[col].dtype not in ['float64','int64']]\n",
    "        \n",
    "            \n",
    "    def impute_missing_simple(self, num_miss_val=np.NaN, num_strat='mean', \n",
    "                               num_fill_val=None, cat_miss_val=None, \n",
    "                               cat_strat='most_frequent', cat_fill_val=None):        \n",
    "        \"\"\"\n",
    "        Impute missing values. Use different strategies for numerical\n",
    "        and categorical variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.impute.SimpleImputer parameters.        \n",
    "        num_ : are for numerical variables.\n",
    "        cat_ : are for categorical variables.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        This method should only be used once by instance.\n",
    "        \"\"\"        \n",
    "        if self.num_cols:\n",
    "            self.data[self.num_cols] = \\\n",
    "                SimpleImputer(missing_values=num_miss_val, strategy=num_strat, \n",
    "                              fill_value=num_fill_val).fit_transform(self.data[self.num_cols])\n",
    "        \n",
    "        if self.cat_cols:\n",
    "            self.data[self.cat_cols] = \\\n",
    "                SimpleImputer(missing_values=cat_miss_val, strategy=cat_strat, \n",
    "                              fill_value=cat_fill_val).fit_transform(self.data[self.cat_cols])\n",
    "            \n",
    "        if self.dic_cols:\n",
    "            self.data[self.dic_cols] = \\\n",
    "                SimpleImputer(missing_values=cat_miss_val, strategy=cat_strat, \n",
    "                              fill_value=cat_fill_val).fit_transform(self.data[self.dic_cols])\n",
    "           \n",
    "        \n",
    "    def transform_cat_col(self):\n",
    "        \"\"\"\n",
    "        Transform categorical variable in dummies using \n",
    "        sklearn.preprocessing.OneHotEncoder.  \n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after: seperate_num_cat().\n",
    "        This method should only be used once by instance.\n",
    "        \"\"\" \n",
    "        if self.cat_cols:\n",
    "            encoder = OneHotEncoder(sparse=False, handle_unknown='ignore' \n",
    "                                   ).fit(self.data[self.cat_cols])\n",
    "            self.one_hot_cat_cols = list(encoder.get_feature_names(self.cat_cols))\n",
    "            self.data[self.one_hot_cat_cols] = encoder.transform(self.data[self.cat_cols])\n",
    "            \n",
    "    \n",
    "    def transform_num_col(self, scaler):\n",
    "        \"\"\"\n",
    "        Transform numerical variables using two strategies.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scaler : string\n",
    "                 Use sklearn.preprocessing.StandardScaler if \"StandardScaler\"\n",
    "                 Use sklearn.preprocessing.MinMaxScaler if \"MinMaxScaler\"\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after: seperate_num_cat().\n",
    "        This method should only be used once by instance.\n",
    "        \"\"\" \n",
    "        if self.num_cols:\n",
    "            if scaler == \"MinMaxScaler\":\n",
    "                self.data[self.num_cols] = \\\n",
    "                MinMaxScaler().fit_transform(self.data[self.num_cols])\n",
    "                \n",
    "            elif scaler == \"StandardScaler\":\n",
    "                self.data[self.num_cols] = \\\n",
    "                StandardScaler().fit_transform(self.data[self.num_cols])\n",
    "            else:\n",
    "                print(scaler + \" is not in scaler choice\")\n",
    "                return\n",
    "    \n",
    "    \n",
    "    def drop_untransform_cat(self):\n",
    "        \"\"\"\n",
    "        Drop untransformed categorical variables from the dataset.  \n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after: transform_cat_col().\n",
    "        This method should only be used once by instance.\n",
    "        \"\"\"         \n",
    "        if self.cat_cols:\n",
    "            self.data      = self.data[self.num_cols + self.one_hot_cat_cols]\n",
    "            self.var_names = self.num_cols + self.one_hot_cat_cols\n",
    "            self.cat_cols  = None\n",
    "                \n",
    "    \n",
    "    def create_Y_X(self, y_name):\n",
    "        \"\"\"\n",
    "        Seperate outcome and predictors variables for estimators.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_name : string\n",
    "                 Outcome variable\n",
    "        \"\"\"        \n",
    "        if y_name not in self.var_names :            \n",
    "            print(f'Variable {y_name}, is not in the dataset')\n",
    "            return \n",
    "        \n",
    "        self.y_name  = y_name\n",
    "        self.y       = self.data[y_name]\n",
    "        self.X       = self.data.drop([y_name], axis=1)\n",
    "        self.X_names = list(self.X.columns)\n",
    "        \n",
    "      \n",
    "    def create_Y_X_first(self):\n",
    "        \"\"\"\n",
    "        Seperate outcome and predictors variables for estimators.\n",
    "        Use first variable in the dataset as outcome variable.\n",
    "        \"\"\"        \n",
    "        self.y_name  = self.data.columns[0]\n",
    "        self.y       = self.data.iloc[:,0]\n",
    "        self.X       = self.data.iloc[:,1:]\n",
    "        self.X_names = list(self.X.columns)\n",
    "        \n",
    "    \n",
    "    def _compute_y_test_var(self):\n",
    "        \"\"\"\n",
    "        Compute variance of the outcome variable in the test subset.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after spliting the dataset into train and test subsets.        \n",
    "        \"\"\"      \n",
    "        self.y_test_var = self.y_test.var()\n",
    "    \n",
    "    \n",
    "    def split_random(self, size_test):\n",
    "        \"\"\"      \n",
    "        Split dataset into random train and test subsets.\n",
    "        Use sklearn.model_selection.train_test_split.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size_test : int or float\n",
    "                    If float, should be between 0.0 and 1.0 and represent the proportion \n",
    "                    of the dataset to include in the test split. \n",
    "                    If int, represents the absolute number of test samples. \n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after creating outcome and predictors variables.\n",
    "        \"\"\"        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=size_test)\n",
    "        \n",
    "        self._compute_y_test_var()\n",
    "    \n",
    "    def split_random_seeded(self, size_test, seed):\n",
    "        \"\"\"      \n",
    "        Split dataset into random train and test subsets using a seed for reproducible output.\n",
    "        Use sklearn.model_selection.train_test_split.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size_test : int or float\n",
    "                    If float, should be between 0.0 and 1.0 and represent the proportion \n",
    "                    of the dataset to include in the test split. \n",
    "                    If int, represents the absolute number of test samples.\n",
    "        seed      : int\n",
    "                    Seed used.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after creating outcome and predictors variables.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=size_test, random_state=seed)\n",
    "        \n",
    "        self._compute_y_test_var()\n",
    "        \n",
    "    def split_no_shuffle(self, test_size):\n",
    "        \"\"\"      \n",
    "        Split dataset without shuffling int train and test subsets.\n",
    "        Use sklearn.model_selection.train_test_split.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size_test : int or float\n",
    "                    If float, should be between 0.0 and 1.0 and represent the proportion \n",
    "                    of the dataset to include in the test split. \n",
    "                    If int, represents the absolute number of test samples.    \n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after creating outcome and predictors variables.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, shuffle = False)     \n",
    "        \n",
    "        self._compute_y_test_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlModel:\n",
    "    \"\"\"\n",
    "    Class taking for imput a DataModel and using different ML estimators\n",
    "    to make predictions and outputting results.\n",
    "    \n",
    "    Warnings\n",
    "    ----------\n",
    "    The class does no validation for inputs in its initialization or in its methods. \n",
    "    It also does not ensure that the steps are performed in the correct order.\n",
    "    There is no exception handling.\n",
    "    \"\"\"\n",
    "            \n",
    "    def __init__(self, DataModel):\n",
    "        \"\"\"\n",
    "        Initialize an instance of this class.        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        DataModel  : DataModel\n",
    "                     Complete dataset preprocessed for ML estimators\n",
    "        \"\"\"       \n",
    "        self.DataModel  = DataModel                   # DataModel of instance\n",
    "        \n",
    "        self.model_name = self.DataModel.model_name   # Name of model   \n",
    "        self.var_names  = self.DataModel.var_names    # Names of the variables\n",
    "        self.data       = self.DataModel.data         # Dataset for estimators\n",
    "        \n",
    "        self.y            = self.DataModel.y          # Outcome variable          \n",
    "        self.y_test       = self.DataModel.y_test     # Test set of y\n",
    "        self.y_train      = self.DataModel.y_train    # Train set of y\n",
    "        self.y_train_full = None                      # Train set of y (untouched)\n",
    "        self.y_test_var   = self.DataModel.y_test_var # Variance of test set of y\n",
    "        self.y_name       = self.DataModel.y_name     # Names of outcome variable\n",
    "        \n",
    "        self.X            = self.DataModel.X          # Predictors variables\n",
    "        self.X_test       = self.DataModel.X_test     # Test set of X\n",
    "        self.X_train      = self.DataModel.X_train    # Train set of X\n",
    "        self.X_train_full = None                      # Train set of X (untouched)\n",
    "        self.X_names      = self.DataModel.X_names    # Names of predictors variables\n",
    "        self.X_top_names  = None                      # Names of top predictors variables\n",
    "      \n",
    "        self.method       = None        # Selected method\n",
    "        self.method_type  = None        # Selected method type\n",
    "        \n",
    "        self.cv_best_param      = None  # Best parameters chosen by cross-validation\n",
    "        self.feature_importance = None  # Feature importance of predictors\n",
    "        self.feature_import_top = None  # Top feature importance of predictors\n",
    "        self.mse_var            = None  # Mean squared error of prediction / test y variance\n",
    "        self.mse                = None  # Mean squared error of prediction \n",
    "        self.R2                 = None  # Score(R2) of prediction \n",
    "        self.alphas             = None  # List of alphas for bagging Lasso        \n",
    "        \n",
    "        self.methods = [\"lasso_ridge_elastic\", \"lasso_ridge_CV\", \"elastic_net_CV\",\n",
    "                        \"tree_forest\", \"neural_network\"] # List of method types\n",
    "        \n",
    "        \n",
    "    def get_mse_var(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        mse_var : float\n",
    "                  Mean squared error of prediction\n",
    "        \"\"\"\n",
    "        return self.mse_var\n",
    "    \n",
    "            \n",
    "    def get_cv_best_param(self): \n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        cv_best_param : Dictionary\n",
    "                        Best parameters chosen by cross-validation\n",
    "        \"\"\"\n",
    "        return self.cv_best_param\n",
    "    \n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        feature_importance : ndarray of shape (n_features)\n",
    "                             Feature importance of predictors\n",
    "        \"\"\"\n",
    "        return self.feature_importance\n",
    "    \n",
    "    \n",
    "    def get_feature_importance_top(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        feature_import_top : List(float)\n",
    "                             Top feature importance of predictors\n",
    "        \"\"\"\n",
    "        return self.feature_import_top\n",
    "        \n",
    "    \n",
    "    def get_feature_name(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        X_names : List(string)\n",
    "                  Names of predictors variables\n",
    "        \"\"\"\n",
    "        return self.X_names\n",
    "    \n",
    "    \n",
    "    def get_feature_name_top(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        X_top_names : List(string)\n",
    "                      Names of top predictors variables\n",
    "        \"\"\"\n",
    "        return self.X_top_names\n",
    "    \n",
    "    \n",
    "    def get_bagging_lasso_alphas(self):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ----------\n",
    "        alphas : List(float)\n",
    "                 List of alphas for bagging Lasso\n",
    "        \"\"\"\n",
    "        return self.alphas\n",
    "    \n",
    "    \n",
    "    def select_lasso(self, alpha=1.0, fit_intercept=True, precompute=False, max_iter=1000, \n",
    "                     tol=1e-4, warm_start=False, random_state=None, selection=\"cyclic\"):\n",
    "        \"\"\"\n",
    "        Select lasso as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.Lasso parameters.        \n",
    "        \"\"\"\n",
    "        self.method = \\\n",
    "            Lasso(alpha=alpha, fit_intercept=fit_intercept, precompute=precompute, \n",
    "                  max_iter=max_iter, tol=tol, warm_start=warm_start, random_state= \n",
    "                  random_state, selection=selection)\n",
    "        \n",
    "        self.method_type = self.methods[0]\n",
    "        \n",
    "        \n",
    "    def select_lassoCV(self, eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True, \n",
    "                       precompute='auto', max_iter=1000, tol=1e-4, cv=5, verbose=0, \n",
    "                       n_jobs=None, random_state=None, selection=\"cyclic\"):\n",
    "        \"\"\"\n",
    "        Select lasso with built-in cross validation as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.LassoCV parameters.        \n",
    "        \"\"\"\n",
    "        self.method = \\\n",
    "            LassoCV(eps=eps, n_alphas=n_alphas, alphas=alphas, fit_intercept=fit_intercept, \n",
    "                    precompute=precompute, max_iter=max_iter, tol=tol, cv=cv, verbose= \n",
    "                    verbose, n_jobs=n_jobs, random_state=random_state, selection=selection)\n",
    "        \n",
    "        self.method_type = self.methods[1]\n",
    "        \n",
    "    \n",
    "    def select_ridge(self, alpha=1.0, fit_intercept=True, max_iter=None, solver='auto', \n",
    "                     tol=1e-3, random_state=None):\n",
    "        \"\"\"\n",
    "        Select ridge as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.Ridge parameters.        \n",
    "        \"\"\"    \n",
    "        self.method = \\\n",
    "            Ridge(alpha=alpha, fit_intercept=fit_intercept, solver=solver, \n",
    "                  max_iter=max_iter, tol=tol, random_state=random_state)\n",
    "        \n",
    "        self.method_type = self.methods[0]\n",
    "        \n",
    "        \n",
    "    def select_ridgeCV(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, gcv_mode='auto', \n",
    "                       scoring=None, cv=None):\n",
    "        \"\"\"\n",
    "        Select ridge with built-in cross validation as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.RidgeCV parameters.        \n",
    "        \"\"\"    \n",
    "        self.method = \\\n",
    "            RidgeCV(alphas=alphas, fit_intercept=fit_intercept, gcv_mode=gcv_mode, \n",
    "                    scoring=scoring, cv=cv)\n",
    "        \n",
    "        self.method_type = self.methods[1]\n",
    "        \n",
    "        \n",
    "    def select_elasticNet(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, \n",
    "                          precompute=False, max_iter=1000, tol=1e-4, warm_start=False, \n",
    "                          random_state=None, selection=\"cyclic\"):\n",
    "        \"\"\"\n",
    "        Select elastic net as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.ElasticNet parameters.        \n",
    "        \"\"\"\n",
    "        self.method = \\\n",
    "            ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=fit_intercept, \n",
    "                       precompute=precompute, max_iter=max_iter, tol=tol, \n",
    "                       warm_start=warm_start, random_state= random_state, \n",
    "                       selection=selection)\n",
    "        \n",
    "        self.method_type = self.methods[0]\n",
    "        \n",
    "        \n",
    "    def select_elasticNetCV(self, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None, \n",
    "                            fit_intercept=True, precompute='auto', max_iter=1000, tol=1e-4, \n",
    "                            cv=5, verbose=0, n_jobs=None, random_state=None, \n",
    "                            selection=\"cyclic\"):        \n",
    "        \"\"\"\n",
    "        Select elastic net with built-in cross validation as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.linear_model.ElasticNetCV parameters.        \n",
    "        \"\"\" \n",
    "        self.method = \\\n",
    "            ElasticNetCV(l1_ratio=l1_ratio, eps=eps, n_alphas=n_alphas, alphas=alphas, \n",
    "                         fit_intercept=fit_intercept, precompute=precompute, \n",
    "                         max_iter=max_iter, tol=tol, cv=cv, verbose=verbose, n_jobs=n_jobs, \n",
    "                         random_state=random_state, selection=selection)\n",
    "        \n",
    "        self.method_type = self.methods[2]\n",
    "        \n",
    "        \n",
    "    def select_regression_tree(self, criterion=\"mse\", max_depth=None, max_features=None, \n",
    "                               random_state=None, splitter=\"best\", min_samples_leaf=1, \n",
    "                               min_samples_split=2):        \n",
    "        \"\"\"\n",
    "        Select regression tree as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.tree.DecisionTreeRegressor parameters.       \n",
    "        \"\"\"\n",
    "        self.method = \\\n",
    "            DecisionTreeRegressor(criterion=criterion, max_depth=max_depth, max_features= \n",
    "                                  max_features, random_state=random_state, splitter=splitter,\n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  min_samples_split=min_samples_split) \n",
    "        \n",
    "        self.method_type = self.methods[3]\n",
    "        \n",
    "        \n",
    "    def select_boosted_trees(self, loss=\"ls\", learning_rate=0.1, n_estimators=100, \n",
    "                             subsample=1.0, criterion=\"friedman_mse\", max_depth=3, \n",
    "                             min_samples_leaf=1, min_samples_split=2, random_state=None, \n",
    "                             max_features=None, alpha=0.9, verbose=0, warm_start=False, \n",
    "                             validation_fraction=0.1, n_iter_no_change=None, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Select boosted trees (regressor gradient boosting) as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.ensemble.GradientBoostingRegressor parameters.       \n",
    "        \"\"\"    \n",
    "        self.method = \\\n",
    "            GradientBoostingRegressor(loss=loss, learning_rate=learning_rate, n_estimators=\n",
    "                                      n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                      min_samples_leaf=min_samples_leaf, min_samples_split=\n",
    "                                      min_samples_split, random_state=random_state, \n",
    "                                      max_features=max_features, alpha=alpha, verbose=verbose,\n",
    "                                      subsample=subsample, warm_start=warm_start, tol=tol,\n",
    "                                      validation_fraction=validation_fraction,\n",
    "                                      n_iter_no_change=n_iter_no_change) \n",
    "        \n",
    "        self.method_type = self.methods[3]\n",
    "        \n",
    "    \n",
    "    def select_random_forest(self, n_estimators=100, criterion=\"mse\", \n",
    "                             max_depth=None, max_features=\"auto\", bootstrap=False, \n",
    "                             oob_score=False, n_jobs=None, random_state=None, verbose=0, \n",
    "                             max_samples=None, min_samples_leaf=1, min_samples_split=2):        \n",
    "        \"\"\"\n",
    "        Select random forest (regressor) as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.ensemble.RandomForestRegressor parameters.        \n",
    "        \"\"\"\n",
    "        self.method = \\\n",
    "            RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, \n",
    "                                 max_depth=max_depth, max_features=max_features, bootstrap= \n",
    "                                 bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state = \n",
    "                                 random_state, verbose=verbose, max_samples=max_samples, \n",
    "                                 min_samples_leaf=min_samples_leaf, min_samples_split= \n",
    "                                 min_samples_split) \n",
    "        \n",
    "        self.method_type = self.methods[3]  \n",
    "        \n",
    "    \n",
    "    def select_MLP_regressor(self, hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
    "                             alpha=0.0001, learning_rate='constant', learning_rate_init=0.001,\n",
    "                             max_iter=200, shuffle=True, random_state=None, tol=1e-4, \n",
    "                             warm_start=False, early_stopping=False,validation_fraction=0.1, \n",
    "                             n_iter_no_change=10, batch_size='auto'):\n",
    "        \"\"\"\n",
    "        Select neural network (multi-layer perceptron regressor) as ML estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Represent sklearn.neural_network.MLPRegressor parameters.        \n",
    "        \"\"\"        \n",
    "        self.method = \\\n",
    "            MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation,\n",
    "                         solver=solver, alpha=alpha, learning_rate=learning_rate, \n",
    "                         learning_rate_init=learning_rate_init, max_iter=max_iter, shuffle= \\\n",
    "                         shuffle, random_state=random_state, tol=tol, warm_start=warm_start,\n",
    "                         early_stopping=early_stopping, validation_fraction=validation_fraction,\n",
    "                         n_iter_no_change=n_iter_no_change, batch_size=batch_size)\n",
    "        \n",
    "        self.method_type = self.methods[4]   \n",
    "        \n",
    "        \n",
    "    def _generate_bootstrap_data(self, n=None, frac=None, replace=False, weights=None,\n",
    "                                 axis=None, ignore_index=False, random_state=None):\n",
    "        \"\"\"\n",
    "        Genrerate a bootstraped dataset for bagging Lasso\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frac    : float\n",
    "                  % of orignial dataset to make the new one\n",
    "        replace : boolean\n",
    "                  Sampling with replacement if True\n",
    "                  \n",
    "        The rest are others pandas.DataFrame.sample parameters.     \n",
    "        \"\"\"        \n",
    "        self.X_train = \\\n",
    "            self.X_train_full.sample(n=n, frac=frac, replace=replace, weights=weights, axis=axis,\n",
    "                                     ignore_index=ignore_index, random_state=random_state)\n",
    "        \n",
    "        self.y_train = \\\n",
    "            self.y_train_full.sample(n=n, frac=frac, replace=replace, weights=weights, axis=axis,\n",
    "                                     ignore_index=ignore_index, random_state=random_state)\n",
    "       \n",
    "    \n",
    "    def bagging_lasso(self, repetition=200, predefined_sampling=True, print_res=True,\n",
    "                      n=None, frac=None, replace=False, weights=None, random_state=None,\n",
    "                      eps=1e-3, n_alphas=100, alphas=None, fit_intercept=True,\n",
    "                      precompute='auto', max_iter=1000, tol=1e-4, cv=5, verbose=0, n_jobs=None, \n",
    "                      selection=\"cyclic\"):\n",
    "        \"\"\"\n",
    "        Compute a baggin lasso as ML estimator. This estimators iterate a chosen\n",
    "        number of time. Each iteration it takes a bootstrapped dataset and does a\n",
    "        LassoCV. Output the mean prediction of all iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        repetition          : int\n",
    "                              number of iteration\n",
    "        predefined_sampling : boolean\n",
    "                              Use a predefined pattern from a seed for reproducible \n",
    "                              output if True. If False, use random seed for each bootstrap.\n",
    "        print_res           : boolean\n",
    "                              Print results on console if True\n",
    "        frac                : float\n",
    "                              % of orignial dataset to make the new one\n",
    "        replace             : boolean\n",
    "                              Sampling with replacement if True\n",
    "                              \n",
    "        The rest are others pandas.DataFrame.sample and\n",
    "        sklearn.linear_model.LassoCV parameters.        \n",
    "                \n",
    "        Return\n",
    "        ----------\n",
    "        mse     : float\n",
    "                  Mean MSE\n",
    "        mse_var : float\n",
    "                  Mean MSE/y test variance\n",
    "        R2      : float\n",
    "                  Mean score(R2)\n",
    "        alphas  : List(float)\n",
    "                  List of alphas for each iteration              \n",
    "        \"\"\"  \n",
    "        self.alphas = []\n",
    "        mse_list    = []\n",
    "        R2_list     = []\n",
    "                \n",
    "        if random_state is not None: \n",
    "            new_random_state = random_state + ((repetition + 5)*10)\n",
    "        else: \n",
    "            new_random_state = random.randint(0,999999999)\n",
    "        \n",
    "        self.X_train_full = copy.deepcopy(self.X_train)\n",
    "        self.y_train_full = copy.deepcopy(self.y_train)\n",
    "        \n",
    "        for i in range(repetition):\n",
    "            \n",
    "            if predefined_sampling:\n",
    "                new_random_state -= 10\n",
    "            else:\n",
    "                new_random_state = random.randint(0,999999999)\n",
    "\n",
    "            self._generate_bootstrap_data(n=n, frac=frac, replace=replace, weights=weights,\n",
    "                                          random_state=new_random_state)\n",
    "\n",
    "            self.select_lassoCV(eps=eps, n_alphas=n_alphas, alphas=alphas, fit_intercept= \\\n",
    "                                fit_intercept, precompute=precompute, max_iter=max_iter,\n",
    "                                tol=tol, cv=cv, verbose=verbose, n_jobs=n_jobs, \n",
    "                                random_state=random_state, selection=selection)\n",
    "            \n",
    "            self.simple_fit()\n",
    "            self.alphas.append(self.cv_best_param.get('alpha'))\n",
    "            \n",
    "            mse_var, mse, R2 = self.train_test_error(print_res=False)\n",
    "            mse_list.append(mse), R2_list.append(R2)\n",
    "            \n",
    "        self.mse     = mean(mse_list)\n",
    "        self.mse_var = self.mse/self.y_test_var\n",
    "        self.R2      = mean(R2_list)\n",
    "        \n",
    "        self.X_train = self.X_train_full\n",
    "        self.y_train = self.y_train_full\n",
    "        \n",
    "        if print_res == True: print(f'MSE/σ2 = {self.mse_var:.5f} | MSE = {self.mse:.5f} | '\\\n",
    "        f'R2(score) = {self.R2:.5f}')\n",
    "        \n",
    "        return self.mse, self.mse_var, self.R2, self.alphas\n",
    "    \n",
    "        \n",
    "    def _set_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Store features importance.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after fitting the model.        \n",
    "        \"\"\"        \n",
    "        met = self.method_type\n",
    "        if met in self.methods[:3]:\n",
    "            self.feature_importance = self.method.coef_ \n",
    "                \n",
    "        if met == self.methods[3]:\n",
    "            self.feature_importance = self.method.feature_importances_ \n",
    "            \n",
    "  \n",
    "    def compute_top_features(self, num=10):\n",
    "        \"\"\"\n",
    "        Compute and store the top best features from importance\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num : int\n",
    "              Number of top features\n",
    "              \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after fitting the model.  \n",
    "        \"\"\"        \n",
    "        self.feature_import_top = []\n",
    "        self.X_top_names        = []\n",
    "    \n",
    "        temp_dict = dict(zip(self.X_names, np.absolute(self.feature_importance)))\n",
    "        temp_list = sorted(temp_dict.items(), key=lambda x: x[1],  reverse=True)\n",
    "        \n",
    "        for n in range(min(num, len(self.X_names))):\n",
    "            self.feature_import_top.append(temp_list[n][1])\n",
    "            self.X_top_names.append(temp_list[n][0])\n",
    "            \n",
    "        \n",
    "    def simple_fit(self):\n",
    "        \"\"\"\n",
    "        Only using sklearn estimatot.fit(X_train,y_train) to train the model.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after selecting the model.\n",
    "        \"\"\"              \n",
    "        self.method.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        if self.method_type == self.methods[1]:\n",
    "            self.cv_best_param = {'alpha': self.method.alpha_}\n",
    "            \n",
    "        if self.method_type == self.methods[2]:\n",
    "            self.cv_best_param = {'alpha': self.method.alpha_, \n",
    "                                  \"l1_ratio\": self.method.l1_ratio_}\n",
    "            \n",
    "        self._set_feature_importance()\n",
    "        \n",
    "        \n",
    "    def cv_hyperpara_fit(self, param_grid, scoring='neg_mean_squared_error', cv=5, \n",
    "                         verbose=0, n_jobs=None, return_train_score=False, replace=True, \n",
    "                         print_res=True):\n",
    "        \"\"\"\n",
    "        Cross validate chosen hyperparameters for estimations using grid search.\n",
    "        Will fit the model with train set and best hyperparameters found.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        param_grid : dict or list(dict)\n",
    "                     Dictionary with parameters names (str) as keys and lists of parameter\n",
    "                     settings to try as values, or a list of such dictionaries, in which \n",
    "                     case the grids spanned by each dictionary in the list are explored. \n",
    "        replace    : boolean\n",
    "                     Replace selected estimator with best estimator if True.\n",
    "        print_res  : boolean\n",
    "                     Print results on console if True.\n",
    "        \n",
    "        The rest are others sklearn.model_selection.GridSearchCV parameters.\n",
    "        \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after selecting the model.        \n",
    "        \"\"\"        \n",
    "        met     = self.method_type\n",
    "        methods = self.methods \n",
    "        \n",
    "        if met == methods[0] or met == methods[3] or met == methods[4]:\n",
    "        \n",
    "            grid_search = GridSearchCV(self.method, param_grid, cv=cv, scoring=scoring, \n",
    "                                       return_train_score=return_train_score, verbose=verbose, \n",
    "                                       n_jobs = n_jobs)   \n",
    "\n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "            if print_res == True: print(grid_search.best_params_)            \n",
    "            if replace   == True: self.method = grid_search.best_estimator_\n",
    "\n",
    "            self.cv_best_param = grid_search.best_params_            \n",
    "        \n",
    "        elif met == methods[1] or met == methods[2]:\n",
    "            print(\"Method already cross validated\") \n",
    "            \n",
    "        self._set_feature_importance()\n",
    "        \n",
    "\n",
    "    def train_test_error(self, print_res=True):\n",
    "        \"\"\"\n",
    "        Compute mean squared error, MSE/(y test variance) ans score(R2) with\n",
    "        test set prediction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        print_res : boolean\n",
    "                    Print results on console if True.\n",
    "                    \n",
    "        Return\n",
    "        ----------\n",
    "        mse     : float\n",
    "                  MSE\n",
    "        mse_var : float\n",
    "                  MSE/y test variance\n",
    "        R2      : float\n",
    "                  Score(R2)\n",
    "                  \n",
    "        Warnings\n",
    "        ----------\n",
    "        Should only be used after fitting the model.\n",
    "        \"\"\"\n",
    "        self.R2      = self.method.score(self.X_test, self.y_test)\n",
    "        predict      = self.method.predict(self.X_test)\n",
    "        self.mse     = mean_squared_error(self.y_test, predict) \n",
    "        self.mse_var = self.mse/self.y_test_var\n",
    "\n",
    "        if print_res == True: print(f'MSE/σ2 = {self.mse_var:.5f} | MSE = {self.mse:.5f} | '\\\n",
    "        f'R2(score) = {self.R2:.5f}')\n",
    "        return self.mse_var, self.mse, self.R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Abalone\n",
    "    colnames   = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', \\\n",
    "                    'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
    "    \n",
    "    fname      = paths[0] + \"abalone.data\"\n",
    "    abalone_raw = pd.read_csv(fname, sep=\",\", names=colnames)\n",
    "    \n",
    "    # Boston Housing    \n",
    "    fname     = paths[0] + \"Boston_Housing.data\"\n",
    "     \n",
    "    colnames  = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', \\\n",
    "                 'TAX', 'PTRATIO']\n",
    "    df3       = pd.read_csv(fname, delim_whitespace=True, skiprows=lambda x: x%2 == 1,\n",
    "                            header=None, names=colnames)\n",
    "     \n",
    "    colnames  = ['B', 'LSTAT', 'MEDV']\n",
    "    df2       = pd.read_csv(fname, delim_whitespace=True, skiprows=lambda x: x%2 == 0,\n",
    "                            header=None, names=colnames)\n",
    "\n",
    "    boston_raw = pd.concat([df3, df2], axis=1)\n",
    "    \n",
    "    # California Housing   \n",
    "    fname          = paths[0] + \"CAhousing.csv\"\n",
    "    california_raw = pd.read_csv(fname, sep=\",\")\n",
    "    \n",
    "    # US Unemployment Rate (h = 1)\n",
    "    fname        = paths[0] + \"US_UnempRate_h1.csv\"\n",
    "    unemploy_raw = pd.read_csv(fname, sep=\",\")\n",
    "    unemploy_raw = unemploy_raw.iloc[: , 1:]\n",
    "    \n",
    "    # US Inflation Rate (h = 1)\n",
    "    fname     = paths[0] + \"US_infla_h1.csv\"\n",
    "    infl_raw = pd.read_csv(fname, sep=\",\")\n",
    "    infl_raw = infl_raw.iloc[: , 1:]\n",
    "    \n",
    "    # White Wine \n",
    "    fname     = paths[0] + \"winequality-white.csv\"\n",
    "    wine_raw = pd.read_csv(fname, sep=\";\")\n",
    "    \n",
    "    # Fish Toxicity \n",
    "    colnames   = ['CIC0', 'SM1_Dz(Z)', 'GATS1i', 'NdsCH', 'NdssC', 'MLOGP', 'reponse(LC50)']\n",
    "    fname      = paths[0] + \"qsar_fish_toxicity.csv\"\n",
    "    fish_raw  = pd.read_csv(fname, sep=\";\", names=colnames)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f'File {fname} don\\'t exist or are unable to open')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pepare names lists for all models\n",
    "data_sets  = [abalone_raw, boston_raw, california_raw, unemploy_raw, infl_raw, \\\n",
    "              wine_raw, fish_raw]\n",
    "\n",
    "DataModel_names = [\"abalone_data\", \"boston_data\", \"california_data\", \"unemploy_data\", \\\n",
    "                   \"infl_data\", \"wine_data\", \"fish_data\"]\n",
    "MlModel_names   = [\"abalone\", \"boston\", \"california\", \"unemploy\", \"infl\", \"wine\", \"fish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone don't contain missing values\n",
      "boston don't contain missing values\n",
      "california contain 207 missing values\n",
      "unemploy don't contain missing values\n",
      "infl don't contain missing values\n",
      "wine don't contain missing values\n",
      "fish don't contain missing values\n"
     ]
    }
   ],
   "source": [
    "# Create DataModel for each dataset and check for missing values\n",
    "i = 0\n",
    "for n in DataModel_names: \n",
    "    globals()[n] = DataModel(data_sets[i], list(data_sets[i]), MlModel_names[i])\n",
    "    i = i+1\n",
    "\n",
    "DataModels = [abalone_data, boston_data, california_data, unemploy_data, \\\n",
    "              infl_data, wine_data, fish_data]\n",
    "\n",
    "for n in DataModels: n.check_missing() \n",
    "for n in DataModels: n.seperate_num_cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values for california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "california don't contain missing values\n"
     ]
    }
   ],
   "source": [
    "california_data.impute_missing_simple()\n",
    "california_data.check_missing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming categorical variables in dummies with one hot encoder\n",
    "#### Transforming numerical variables with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data:\n",
    "for n in DataModels: n.transform_cat_col()   \n",
    "for n in DataModels: n.drop_untransform_cat()\n",
    "for n in DataModels: n.transform_num_col(\"StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y datasets  \n",
    "abalone_data.create_Y_X(\"Rings\")\n",
    "boston_data.create_Y_X(\"MEDV\")\n",
    "california_data.create_Y_X(\"median_house_value\")\n",
    "unemploy_data.create_Y_X(\"y\")\n",
    "infl_data.create_Y_X(\"y\")\n",
    "wine_data.create_Y_X(\"quality\")\n",
    "fish_data.create_Y_X(\"reponse(LC50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed : 256579770\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for the rest of the estimations\n",
    "seed = random.randint(0,999999999)\n",
    "seed = 256579770\n",
    "print(f'Seed : {seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting dataset with random shuffling except for time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data in train and test set\n",
    "models_continous = [unemploy_data, infl_data]\n",
    "size_test = 0.2\n",
    "\n",
    "for n in DataModels:\n",
    "    if n in models_continous:\n",
    "        n.split_no_shuffle(size_test)\n",
    "    else:\n",
    "        n.split_random_seeded(size_test, seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MlModel for each DataModel\n",
    "i = 0\n",
    "for n in MlModel_names: \n",
    "    globals()[n] = MlModel(DataModels[i])\n",
    "    i = i+1\n",
    "\n",
    "MlModels = [abalone, boston, california, unemploy, infl, wine, fish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results lists\n",
    "methods = [\"Lasso\", \"Ridge\", \"Elas_Net\", \"Reg_Tree\", \"Boost_Trees\", \n",
    "           \"RF\", \"NN_(5:100)\", \"NN_(2:5)\", \"Bag_Lasso\"]\n",
    "\n",
    "list_mse_var    = []\n",
    "list_hyperpara  = []\n",
    "list_bag_alphas = []\n",
    "\n",
    "for n in MlModel_names:\n",
    "    name = n + \"_mse_var\"\n",
    "    temp = globals()[name] = []\n",
    "    list_mse_var.append(temp)\n",
    "    \n",
    "    name = n + \"_hyperpara\"\n",
    "    temp = globals()[name] = []\n",
    "    list_hyperpara.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results():\n",
    "    \"\"\"\n",
    "    Store estimations results in lists\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for n in list_mse_var: \n",
    "        n.append(MlModels[i].get_mse_var())\n",
    "        i+=1\n",
    "\n",
    "    i=0\n",
    "    for n in list_hyperpara:\n",
    "        n.append(MlModels[i].get_cv_best_param())\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model estimations and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'alpha': 0.0015998587196060573}\n",
      "MSE/σ2 = 0.45904 | MSE = 0.47993 | R2(score) = 0.54041\n",
      "-----------\n",
      "boston\n",
      "{'alpha': 0.0009102981779915217}\n",
      "MSE/σ2 = 0.23931 | MSE = 0.24678 | R2(score) = 0.75832\n",
      "-----------\n",
      "california\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479.55431756782855, tolerance: 1.6423348494519818\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-08}\n",
      "MSE/σ2 = 0.34614 | MSE = 0.35362 | R2(score) = 0.65378\n",
      "-----------\n",
      "unemploy\n",
      "{'alpha': 0.0625055192527397}\n",
      "MSE/σ2 = 0.26603 | MSE = 0.35169 | R2(score) = 0.72763\n",
      "-----------\n",
      "infl\n",
      "{'alpha': 0.04714866363457394}\n",
      "MSE/σ2 = 1.10811 | MSE = 0.80872 | R2(score) = -0.13449\n",
      "-----------\n",
      "wine\n",
      "{'alpha': 0.006551285568595509}\n",
      "MSE/σ2 = 0.71985 | MSE = 0.78955 | R2(score) = 0.27941\n",
      "-----------\n",
      "fish\n",
      "{'alpha': 1e-08}\n",
      "MSE/σ2 = 0.44751 | MSE = 0.45126 | R2(score) = 0.55002\n"
     ]
    }
   ],
   "source": [
    "# Lasso cross-validation and prediction\n",
    "alphas = np.logspace(-5, 1, 50)\n",
    "alphas_lasso = [1e-8] + alphas.tolist() + np.logspace(0.1, 1, 20).tolist()\n",
    "for n in MlModels: n.select_lassoCV(max_iter=5e+5, random_state=seed, n_jobs=3, \n",
    "                                    alphas=alphas_lasso)\n",
    "    \n",
    "print(\"Lasso\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.simple_fit()\n",
    "    print(n.get_cv_best_param())\n",
    "    n.train_test_error() \n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Lasso best features\n",
    "for n in [california, unemploy, infl]: n.compute_top_features()\n",
    "\n",
    "lasso_california_top_names  = california.get_feature_name_top()\n",
    "lasso_california_top_values = california.get_feature_importance_top()\n",
    "\n",
    "lasso_unemploy_top_names    = unemploy.get_feature_name_top()\n",
    "lasso_unemploy_top_values   = unemploy.get_feature_importance_top()\n",
    "\n",
    "lasso_infl_top_names        = infl.get_feature_name_top()\n",
    "lasso_infl_top_values       = infl.get_feature_importance_top()\n",
    "\n",
    "Lasso_var_import=[lasso_california_top_names, lasso_california_top_values, \n",
    "                  lasso_unemploy_top_names, lasso_unemploy_top_values, \n",
    "                  lasso_infl_top_names,lasso_infl_top_values] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'alpha': 0.7906043210907702}\n",
      "MSE/σ2 = 0.45584 | MSE = 0.47658 | R2(score) = 0.54362\n",
      "-----------\n",
      "boston\n",
      "{'alpha': 8.040131611167856}\n",
      "MSE/σ2 = 0.24070 | MSE = 0.24822 | R2(score) = 0.75692\n",
      "-----------\n",
      "california\n",
      "{'alpha': 0.19306977288832497}\n",
      "MSE/σ2 = 0.34610 | MSE = 0.35358 | R2(score) = 0.65381\n",
      "-----------\n",
      "unemploy\n",
      "{'alpha': 911.1627561154896}\n",
      "MSE/σ2 = 0.25941 | MSE = 0.34293 | R2(score) = 0.73442\n",
      "-----------\n",
      "infl\n",
      "{'alpha': 162.97508346206433}\n",
      "MSE/σ2 = 1.23702 | MSE = 0.90281 | R2(score) = -0.26648\n",
      "-----------\n",
      "wine\n",
      "{'alpha': 48.62601580065353}\n",
      "MSE/σ2 = 0.71757 | MSE = 0.78705 | R2(score) = 0.28170\n",
      "-----------\n",
      "fish\n",
      "{'alpha': 20.09233002565047}\n",
      "MSE/σ2 = 0.44620 | MSE = 0.44993 | R2(score) = 0.55134\n"
     ]
    }
   ],
   "source": [
    "# Ridge cross-validation and prediction\n",
    "alpha_ridge = alphas_lasso + np.logspace(1, 3, 100).tolist()\n",
    "for n in MlModels: n.select_ridgeCV(alphas=alpha_ridge)\n",
    "    \n",
    "print(\"Ridge\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.simple_fit()    \n",
    "    print(n.get_cv_best_param())\n",
    "    n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastict Net\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'alpha': 0.0015998587196060573, 'l1_ratio': 1.0}\n",
      "MSE/σ2 = 0.45904 | MSE = 0.47993 | R2(score) = 0.54041\n",
      "-----------\n",
      "boston\n",
      "{'alpha': 0.026826957952797246, 'l1_ratio': 0.001}\n",
      "MSE/σ2 = 0.24139 | MSE = 0.24893 | R2(score) = 0.75622\n",
      "-----------\n",
      "california\n",
      "{'alpha': 1.3257113655901082e-05, 'l1_ratio': 0.001}\n",
      "MSE/σ2 = 0.34610 | MSE = 0.35358 | R2(score) = 0.65382\n",
      "-----------\n",
      "unemploy\n",
      "{'alpha': 6.46437163249006, 'l1_ratio': 0.001}\n",
      "MSE/σ2 = 0.27311 | MSE = 0.36105 | R2(score) = 0.72039\n",
      "-----------\n",
      "infl\n",
      "{'alpha': 0.04714866363457394, 'l1_ratio': 1.0}\n",
      "MSE/σ2 = 1.10811 | MSE = 0.80872 | R2(score) = -0.13449\n",
      "-----------\n",
      "wine\n",
      "{'alpha': 0.01151395399326447, 'l1_ratio': 0.001}\n",
      "MSE/σ2 = 0.71755 | MSE = 0.78702 | R2(score) = 0.28172\n",
      "-----------\n",
      "fish\n",
      "{'alpha': 0.03556480306223128, 'l1_ratio': 0.001}\n",
      "MSE/σ2 = 0.44595 | MSE = 0.44969 | R2(score) = 0.55158\n"
     ]
    }
   ],
   "source": [
    "# Elastict Net cross-validation and prediction\n",
    "list_l1  = [0.001, .05, .1, .2, .5, .7, .8, .9, .925, .95, .975,.99, 1]\n",
    "alpha_EN = alphas.tolist() + np.logspace(0.1, 1, 20).tolist() + np.logspace(1, 2, 30).tolist()\n",
    "for n in MlModels: n.select_elasticNetCV(max_iter=1e+6, random_state=seed, n_jobs=3, \\\n",
    "                                          alphas=alpha_EN, l1_ratio=list_l1)\n",
    "      \n",
    "print(\"Elastict Net\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.simple_fit()    \n",
    "    print(n.get_cv_best_param())\n",
    "    n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression tree\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'max_depth': 5}\n",
      "MSE/σ2 = 0.54285 | MSE = 0.56756 | R2(score) = 0.45650\n",
      "-----------\n",
      "boston\n",
      "{'max_depth': 9}\n",
      "MSE/σ2 = 0.16404 | MSE = 0.16916 | R2(score) = 0.83433\n",
      "-----------\n",
      "california\n",
      "{'max_depth': 9}\n",
      "MSE/σ2 = 0.27711 | MSE = 0.28310 | R2(score) = 0.72283\n",
      "-----------\n",
      "unemploy\n",
      "{'max_depth': 4}\n",
      "MSE/σ2 = 0.66018 | MSE = 0.87274 | R2(score) = 0.32410\n",
      "-----------\n",
      "infl\n",
      "{'max_depth': 3}\n",
      "MSE/σ2 = 1.16998 | MSE = 0.85387 | R2(score) = -0.19783\n",
      "-----------\n",
      "wine\n",
      "{'max_depth': 4}\n",
      "MSE/σ2 = 0.72313 | MSE = 0.79314 | R2(score) = 0.27613\n",
      "-----------\n",
      "fish\n",
      "{'max_depth': 4}\n",
      "MSE/σ2 = 0.55047 | MSE = 0.55508 | R2(score) = 0.44649\n"
     ]
    }
   ],
   "source": [
    "# Regression tree cross-validation and prediction\n",
    "for n in MlModels: n.select_regression_tree(random_state=seed)\n",
    "\n",
    "depth_list = [i for i in range(1, 100)] \n",
    "param_grid = [{'max_depth': depth_list}]\n",
    "\n",
    "print(\"Regression tree\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.cv_hyperpara_fit(param_grid=param_grid, verbose=0, n_jobs=3)\n",
    "    n.train_test_error()   \n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted trees\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.47674 | MSE = 0.49844 | R2(score) = 0.52269\n",
      "-----------\n",
      "boston\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.08656 | MSE = 0.08927 | R2(score) = 0.91258\n",
      "-----------\n",
      "california\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.15730 | MSE = 0.16070 | R2(score) = 0.84266\n",
      "-----------\n",
      "unemploy\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.38156 | MSE = 0.50441 | R2(score) = 0.60936\n",
      "-----------\n",
      "infl\n",
      "{'learning_rate': 0.05, 'max_depth': 1, 'n_estimators': 500}\n",
      "MSE/σ2 = 1.10178 | MSE = 0.80410 | R2(score) = -0.12801\n",
      "-----------\n",
      "wine\n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.54424 | MSE = 0.59694 | R2(score) = 0.45520\n",
      "-----------\n",
      "fish\n",
      "{'learning_rate': 0.005, 'max_depth': 4, 'n_estimators': 750}\n",
      "MSE/σ2 = 0.47057 | MSE = 0.47451 | R2(score) = 0.52683\n"
     ]
    }
   ],
   "source": [
    "# Boosted trees cross-validation and prediction\n",
    "for n in MlModels: n.select_boosted_trees(random_state=seed)\n",
    "\n",
    "param_grid = [{'max_depth': [i for i in range(1, 5)],\n",
    "               'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "               'n_estimators': [100, 250, 500, 750]}]\n",
    "\n",
    "print(\"Boosted trees\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.cv_hyperpara_fit(param_grid=param_grid, verbose=0, n_jobs=4)\n",
    "    mse_var, mse, r2 = n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'max_features': 0.3333333333333333}\n",
      "MSE/σ2 = 0.48589 | MSE = 0.50800 | R2(score) = 0.51353\n",
      "-----------\n",
      "boston\n",
      "{'max_features': 0.3333333333333333}\n",
      "MSE/σ2 = 0.07624 | MSE = 0.07862 | R2(score) = 0.92300\n",
      "-----------\n",
      "california\n",
      "{'max_features': 0.5}\n",
      "MSE/σ2 = 0.16566 | MSE = 0.16924 | R2(score) = 0.83430\n",
      "-----------\n",
      "unemploy\n",
      "{'max_features': 'sqrt'}\n",
      "MSE/σ2 = 0.31520 | MSE = 0.41669 | R2(score) = 0.67729\n",
      "-----------\n",
      "infl\n",
      "{'max_features': 'sqrt'}\n",
      "MSE/σ2 = 1.23430 | MSE = 0.90082 | R2(score) = -0.26368\n",
      "-----------\n",
      "wine\n",
      "{'max_features': 0.3333333333333333}\n",
      "MSE/σ2 = 0.45221 | MSE = 0.49600 | R2(score) = 0.54733\n",
      "-----------\n",
      "fish\n",
      "{'max_features': 0.3333333333333333}\n",
      "MSE/σ2 = 0.46222 | MSE = 0.46610 | R2(score) = 0.53522\n"
     ]
    }
   ],
   "source": [
    "# Random forest cross-validation and prediction\n",
    "for n in MlModels: n.select_random_forest(n_estimators=500, random_state=seed, n_jobs=4)\n",
    "\n",
    "param_grid = [{'max_features': [1.0, 1/2, 1/3, \"sqrt\"]}]\n",
    "\n",
    "print(\"Random forest\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.cv_hyperpara_fit(param_grid=param_grid, verbose=0, n_jobs=None)\n",
    "    mse_var, mse, r2 = n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Random Forest best features\n",
    "for n in [california, unemploy, infl]: n.compute_top_features()\n",
    "\n",
    "RF_california_top_names  = california.get_feature_name_top()\n",
    "RF_california_top_values = california.get_feature_importance_top()\n",
    "\n",
    "RF_unemploy_top_names    = unemploy.get_feature_name_top()\n",
    "RF_unemploy_top_values   = unemploy.get_feature_importance_top()\n",
    "\n",
    "RF_infl_top_names        = infl.get_feature_name_top()\n",
    "RF_infl_top_values       = infl.get_feature_importance_top()\n",
    "\n",
    "RF_var_import=[RF_california_top_names, RF_california_top_values,\n",
    "               RF_unemploy_top_names, RF_unemploy_top_values,\n",
    "               RF_infl_top_names,RF_infl_top_values]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network (5:100)\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 50}\n",
      "MSE/σ2 = 0.44820 | MSE = 0.46860 | R2(score) = 0.55126\n",
      "-----------\n",
      "boston\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 20}\n",
      "MSE/σ2 = 0.08638 | MSE = 0.08908 | R2(score) = 0.91276\n",
      "-----------\n",
      "california\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 20}\n",
      "MSE/σ2 = 0.19171 | MSE = 0.19585 | R2(score) = 0.80825\n",
      "-----------\n",
      "unemploy\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 10}\n",
      "MSE/σ2 = 0.24945 | MSE = 0.32977 | R2(score) = 0.74461\n",
      "-----------\n",
      "infl\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 50}\n",
      "MSE/σ2 = 0.94119 | MSE = 0.68690 | R2(score) = 0.03640\n",
      "-----------\n",
      "wine\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 10}\n",
      "MSE/σ2 = 0.62520 | MSE = 0.68573 | R2(score) = 0.37417\n",
      "-----------\n",
      "fish\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 10}\n",
      "MSE/σ2 = 0.45896 | MSE = 0.46280 | R2(score) = 0.53851\n"
     ]
    }
   ],
   "source": [
    "# Neural network (5:100) cross-validation and prediction\n",
    "layers = (100, 100, 100, 100, 100)\n",
    "for n in MlModels: n.select_MLP_regressor(hidden_layer_sizes=layers, random_state=seed,\n",
    "                                          early_stopping=True)\n",
    "\n",
    "param_grid = [{'n_iter_no_change': [10, 20, 50, 100],\n",
    "               'learning_rate_init': [0.1, 0.05, 0.01, 0.001]}]\n",
    "\n",
    "print(\"Neural network (5:100)\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.cv_hyperpara_fit(param_grid=param_grid, verbose=0, n_jobs=3)\n",
    "    mse_var, mse, r2 = n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network (2:5)\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "{'learning_rate_init': 0.1, 'n_iter_no_change': 100}\n",
      "MSE/σ2 = 0.44296 | MSE = 0.46312 | R2(score) = 0.55651\n",
      "-----------\n",
      "boston\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 50}\n",
      "MSE/σ2 = 0.14165 | MSE = 0.14607 | R2(score) = 0.85695\n",
      "-----------\n",
      "california\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 100}\n",
      "MSE/σ2 = 0.23282 | MSE = 0.23785 | R2(score) = 0.76712\n",
      "-----------\n",
      "unemploy\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 20}\n",
      "MSE/σ2 = 0.57572 | MSE = 0.76109 | R2(score) = 0.41057\n",
      "-----------\n",
      "infl\n",
      "{'learning_rate_init': 0.01, 'n_iter_no_change': 10}\n",
      "MSE/σ2 = 2.75116 | MSE = 2.00785 | R2(score) = -1.81666\n",
      "-----------\n",
      "wine\n",
      "{'learning_rate_init': 0.001, 'n_iter_no_change': 50}\n",
      "MSE/σ2 = 0.67287 | MSE = 0.73802 | R2(score) = 0.32644\n",
      "-----------\n",
      "fish\n",
      "{'learning_rate_init': 0.05, 'n_iter_no_change': 10}\n",
      "MSE/σ2 = 0.45061 | MSE = 0.45438 | R2(score) = 0.54690\n"
     ]
    }
   ],
   "source": [
    "# Neural network (2:5) cross-validation and prediction\n",
    "layers = (5, 5)\n",
    "for n in MlModels: n.select_MLP_regressor(hidden_layer_sizes=layers, random_state=seed,\n",
    "                                          early_stopping=True, max_iter=1000)\n",
    "\n",
    "param_grid = [{'n_iter_no_change': [10, 20, 50, 100],\n",
    "               'learning_rate_init': [0.1, 0.05, 0.01, 0.001]}]\n",
    "\n",
    "print(\"Neural network (2:5)\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.cv_hyperpara_fit(param_grid=param_grid, verbose=0, n_jobs=3)\n",
    "    mse_var, mse, r2 = n.train_test_error()\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging lasso\n",
      "============\n",
      "-----------\n",
      "abalone\n",
      "MSE/σ2 = 0.45696 | MSE = 0.47775 | R2(score) = 0.54250\n",
      "-----------\n",
      "boston\n",
      "MSE/σ2 = 0.26489 | MSE = 0.27316 | R2(score) = 0.73249\n",
      "-----------\n",
      "california\n",
      "MSE/σ2 = 0.34694 | MSE = 0.35444 | R2(score) = 0.65298\n",
      "-----------\n",
      "unemploy\n",
      "MSE/σ2 = 0.40394 | MSE = 0.53400 | R2(score) = 0.58644\n",
      "-----------\n",
      "infl\n",
      "MSE/σ2 = 1.32397 | MSE = 0.96626 | R2(score) = -0.35549\n",
      "-----------\n",
      "wine\n",
      "MSE/σ2 = 0.72125 | MSE = 0.79108 | R2(score) = 0.27802\n",
      "-----------\n",
      "fish\n",
      "MSE/σ2 = 0.45349 | MSE = 0.45729 | R2(score) = 0.54401\n"
     ]
    }
   ],
   "source": [
    "# Bagging lasso cross-validation and prediction\n",
    "bagging_alpha = alphas = np.logspace(-6, 1, 280).tolist() + np.logspace(0.1, 1, 20).tolist()\n",
    "print(\"Bagging lasso\\n============\")\n",
    "for n in MlModels:\n",
    "    print(\"-----------\\n\" + n.model_name )\n",
    "    n.bagging_lasso(frac=0.85, replace=True, random_state=seed,\n",
    "                    alphas=bagging_alpha, max_iter=50000, n_jobs=3)\n",
    "    \n",
    "    list_bag_alphas.append(n.get_bagging_lasso_alphas())\n",
    "    \n",
    "store_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store and save results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep  = \",\"\n",
    "path = paths[1]\n",
    "\n",
    "# Bagging Lasso alphas\n",
    "\n",
    "file_name = \"bag_lambda.txt\"\n",
    "fname     = path + file_name\n",
    "\n",
    "with open(fname, 'w',  encoding=\"utf-8\") as f:\n",
    "    text = \"\"\n",
    "    for n in MlModel_names: \n",
    "        text += (\"%s\" % n) + sep\n",
    "    text = text[:-len(sep)]\n",
    "    f.write(text + \"\\n\")\n",
    "    \n",
    "    for i in range(len(list_bag_alphas[0])):\n",
    "        text = \"\"\n",
    "        for n in list_bag_alphas:\n",
    "            text += (\"%s\" % n[i]) + sep\n",
    "        text = text[:-len(sep)]\n",
    "        f.write(text + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "# MSE/σ2\n",
    "file_name = \"mse_var.txt\"\n",
    "    \n",
    "fname = path + file_name\n",
    "\n",
    "with open(fname, 'w',  encoding=\"utf-8\") as f:\n",
    "    text = sep\n",
    "    for n in methods: \n",
    "        text += (\"%s\" % n) + sep\n",
    "    text = text[:-len(sep)]\n",
    "    f.write(text + \"\\n\")\n",
    "\n",
    "    i=0\n",
    "    for item in list_mse_var:\n",
    "        text = MlModel_names[i] + sep\n",
    "        for n in item:        \n",
    "            text += (\"%s\" % n) + sep\n",
    "        text = text[:-len(sep)]\n",
    "        f.write(text + \"\\n\")\n",
    "        i+=1\n",
    "        \n",
    "# Hyperparameter CV\n",
    "file_name = \"hyperpara.txt\"    \n",
    "fname = path + file_name\n",
    "\n",
    "hyper_para = [\"L_lambda \", \"R_lambda \", \"EN_lambda \", \"EN_alpha \", \"RT_depth \", \n",
    "              \"BT_learn_rate \", \"BT_depth \", \"BT_#tree \", \"RF_mtry \", \"NN_5:100_learn_rate \", \n",
    "              \"NN_5:100_early_stop \", \"NN_2:5_learn_rate \", \"NN_2:5_early_stop \"]\n",
    "\n",
    "with open(fname, 'w',  encoding=\"utf-8\") as f:\n",
    "    text = sep\n",
    "    for n in hyper_para: \n",
    "        text += (\"%s\" % n) + sep\n",
    "    text = text[:-len(sep)]\n",
    "    f.write(text + \"\\n\")\n",
    "    \n",
    "    i=0\n",
    "    for item in list_hyperpara:\n",
    "        text = MlModel_names[i] + sep\n",
    "        text += (\"%s\" % item[0].get('alpha')) + sep\n",
    "        text += (\"%s\" % item[1].get('alpha')) + sep\n",
    "        text += (\"%s\" % item[2].get('alpha')) + sep\n",
    "        text += (\"%s\" % item[2].get('l1_ratio')) + sep\n",
    "        text += (\"%s\" % item[3].get('max_depth')) + sep\n",
    "        text += (\"%s\" % item[4].get('learning_rate')) + sep\n",
    "        text += (\"%s\" % item[4].get('max_depth')) + sep\n",
    "        text += (\"%s\" % item[4].get('n_estimators')) + sep\n",
    "        text += (\"%s\" % item[5].get('max_features')) + sep\n",
    "        text += (\"%s\" % item[6].get('learning_rate_init')) + sep\n",
    "        text += (\"%s\" % item[6].get('n_iter_no_change')) + sep\n",
    "        text += (\"%s\" % item[7].get('learning_rate_init')) + sep\n",
    "        text += (\"%s\" % item[7].get('n_iter_no_change')) + sep\n",
    "        text = text[:-len(sep)]\n",
    "        f.write(text + \"\\n\")\n",
    "        i+=1\n",
    "\n",
    "# Features importances\n",
    "cols_name = []\n",
    "for i in range(1,11): cols_name.append(\"var\" + str(i))   \n",
    "rows_name = [\"California_names\", \"California_values\", \"unemploy_names\", \"unemploy_values\", \n",
    "             \"infl_names\", \"infl_values\"]\n",
    "    \n",
    "file_name   = \"Lasso_coefficients.txt\"    \n",
    "fname_lasso = path + file_name\n",
    "file_name   = \"RF_features_importances.txt\" \n",
    "fname_RF    = path + file_name \n",
    "\n",
    "with open(fname_lasso, 'w',  encoding=\"utf-8\") as a, \\\n",
    "     open(fname_RF, 'w',  encoding=\"utf-8\") as b:\n",
    "        \n",
    "    text_lasso = sep\n",
    "    text_RF    = sep\n",
    "    for n in cols_name: \n",
    "        text_lasso += (\"%s\" % n) + sep\n",
    "        text_RF    += (\"%s\" % n) + sep\n",
    "        \n",
    "    text_lasso = text_lasso[:-len(sep)]\n",
    "    text_RF = text_RF[:-len(sep)]\n",
    "    a.write(text_lasso + \"\\n\")\n",
    "    b.write(text_RF + \"\\n\")\n",
    "    \n",
    "    for i in range(len(Lasso_var_import)):\n",
    "        text_lasso = rows_name[i] + sep\n",
    "        text_RF    = rows_name[i] + sep\n",
    "        \n",
    "        for n in Lasso_var_import[i]:        \n",
    "            text_lasso += (\"%s\" % n) + sep\n",
    "        text_lasso = text_lasso[:-len(sep)]\n",
    "        a.write(text_lasso + \"\\n\")  \n",
    "        \n",
    "        for n in RF_var_import[i]:        \n",
    "            text_RF += (\"%s\" % n) + sep\n",
    "        text_RF = text_RF[:-len(sep)]\n",
    "        b.write(text_RF + \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
